{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 8, 2])"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "torch.manual_seed(1337)\n",
    "\n",
    "B, T, C = 4, 8, 2  # Batch, time (tokens), channels (chars or embedding vector)\n",
    "x = torch.randn(B, T, C)\n",
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x[0]:\n",
      "tensor([[ 0.1808, -0.0700],\n",
      "        [-0.3596, -0.9152],\n",
      "        [ 0.6258,  0.0255],\n",
      "        [ 0.9545,  0.0643],\n",
      "        [ 0.3612,  1.1679],\n",
      "        [-1.3499, -0.5102],\n",
      "        [ 0.2360, -0.2398],\n",
      "        [-0.9211,  1.5433]])\n",
      "x avg:\n",
      "tensor([[ 0.1808, -0.0700],\n",
      "        [-0.0894, -0.4926],\n",
      "        [ 0.1490, -0.3199],\n",
      "        [ 0.3504, -0.2238],\n",
      "        [ 0.3525,  0.0545],\n",
      "        [ 0.0688, -0.0396],\n",
      "        [ 0.0927, -0.0682],\n",
      "        [-0.0341,  0.1332]])\n"
     ]
    }
   ],
   "source": [
    "# We want to look at all tokens previous to the current token and\n",
    "# including the current token.\n",
    "# For now, we're just going to average the values for all\n",
    "# the tokens.  It's not very smart, but okay to start.\n",
    "\n",
    "# This set of loops is super slow and inefficient!\n",
    "xavg = torch.zeros((B, T, C))\n",
    "for batch in range(B):\n",
    "    for token in range(T):\n",
    "        xprev = x[batch, :token+1]  # -> [t, C]\n",
    "        xavg[batch, token] = torch.mean(xprev, 0)\n",
    "\n",
    "print(f\"x[0]:\\n{x[0]}\")\n",
    "print(f\"x avg:\\n{xavg[0]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a\n",
      "tensor([[1.0000, 0.0000, 0.0000],\n",
      "        [0.5000, 0.5000, 0.0000],\n",
      "        [0.3333, 0.3333, 0.3333]])\n",
      "b\n",
      "tensor([[8., 6.],\n",
      "        [6., 4.],\n",
      "        [4., 2.]])\n",
      "c=\n",
      "tensor([[8.0000, 6.0000],\n",
      "        [7.0000, 5.0000],\n",
      "        [6.0000, 4.0000]])\n"
     ]
    }
   ],
   "source": [
    "# Matrix math can be fast on a GPU, so use matracies!\n",
    "\n",
    "# We can have torch make us a triangle of numbers.  If they\n",
    "# were all ones, then multiplying a matrix of numbers by the\n",
    "# triangle of ones would give a sum of all previous items\n",
    "# in the column.  If we also scale the ones down by what\n",
    "# row they are on, then multiplying a matrix will end up\n",
    "# taking an average of previous up to current element.\n",
    "all_ones = torch.ones(3, 3)\n",
    "a = torch.tril(all_ones)  # lower triangle\n",
    "a = a / torch.sum(a, 1, keepdim=True) # Scale down by how many ones there are.\n",
    "b = torch.tensor(((8,6), (6,4), (4,2))).float()\n",
    "c = a @ b\n",
    "print('a')\n",
    "print(a)\n",
    "print('b')\n",
    "print(b)\n",
    "print('c=')\n",
    "print(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.1808, -0.0700],\n",
      "        [-0.0894, -0.4926],\n",
      "        [ 0.1490, -0.3199],\n",
      "        [ 0.3504, -0.2238],\n",
      "        [ 0.3525,  0.0545],\n",
      "        [ 0.0688, -0.0396],\n",
      "        [ 0.0927, -0.0682],\n",
      "        [-0.0341,  0.1332]])\n",
      "equal?  True\n"
     ]
    }
   ],
   "source": [
    "# Let's use the trick.\n",
    "wei = torch.tril(torch.ones(T, T))\n",
    "wei = wei / wei.sum(1, keepdim=True)\n",
    "\n",
    "#  (T, T) @ (B, T, C) --> pytorch will auto-scale wei by adding\n",
    "# a leading B dimension.  So now (B, T, T) @ (B, T, C) --> (B, T, C)\n",
    "xavg2 = wei @ x\n",
    "                 \n",
    "# This works because torch does a batch by batch matrix multiply of (T, T) x (T, C)\n",
    "# just like the (3, 3) x (3, 2) above.\n",
    "print(xavg2[0])\n",
    "print(\"equal? \", torch.allclose(xavg, xavg2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "equal3? True\n"
     ]
    }
   ],
   "source": [
    "# Yet another trick for the averages.  We can use softmax.  Softmax can look\n",
    "# at a row of numbers and it gives the probability for a given number, in\n",
    "# context to all the others.  For a simple row of numbers, it's just the average.\n",
    "# Example: 1, 0, 0, 0, 0 ->  1.0,   0, 0, 0, 0\n",
    "#          1, 1, 0, 0, 0 ->  0.5, 0.5, 0, 0, 0  etc\n",
    "# Let's try it.\n",
    "tril = torch.tril(torch.ones(T, T))\n",
    "wei2 = torch.zeros((T, T))\n",
    "wei2 = wei2.masked_fill(tril == 0, float('-inf'))\n",
    "wei2 = F.softmax(wei2, dim=-1)\n",
    "xavg3 = wei2 @ x\n",
    "print(\"equal3?\", torch.allclose(xavg, xavg3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New x tensor([641, 475, 438, 170, 611, 347, 184, 203])\n"
     ]
    }
   ],
   "source": [
    "# Version 4: Self-attention.\n",
    "# Here is where attention gets wild.  Instead of starting wei as zeros, we want to\n",
    "# be smarter.  We will build 2 linear layers.  One called key, one called query.\n",
    "# We have no idea what they will be, they need to be trained.  But they will be\n",
    "# based on how many embeddings are possible.  Let's redefine them now.\n",
    "B, T, C = 4, 8, 32  # So 32 embedding dimensionality.  4 batches of 8 tokens.\n",
    "x = torch.randn(B, T, C)\n",
    "tokens = torch.randint(0,1000,(8,))\n",
    "print(\"New x\", tokens)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "wei[0]:\n",
      "tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.7552, 0.2448, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.6585, 0.2948, 0.0467, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.5087, 0.3908, 0.0152, 0.0854, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.4088, 0.4924, 0.0443, 0.0049, 0.0495, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0061, 0.0300, 0.5775, 0.0287, 0.0047, 0.3530, 0.0000, 0.0000],\n",
      "        [0.0276, 0.0179, 0.8380, 0.0565, 0.0049, 0.0467, 0.0083, 0.0000],\n",
      "        [0.0143, 0.1380, 0.0144, 0.3662, 0.0086, 0.0912, 0.0092, 0.3581]],\n",
      "       grad_fn=<SelectBackward0>)\n",
      "wei shape: torch.Size([4, 8, 8])\n",
      "out shape: torch.Size([4, 8, 32])\n"
     ]
    }
   ],
   "source": [
    "# Make a single head for self-attention.\n",
    "head_size = 16\n",
    "key   = nn.Linear(C, head_size, bias=False) # map 32 inputs to 16 outputs using lots of neurons\n",
    "query = nn.Linear(C, head_size, bias=False) # map 32 inputs to 16 outputs using lots of neurons\n",
    "k = key(x)    # (B, T, 16) ie we did the neural net from 32 -> 16\n",
    "q = query(x)  # (B, T, 16) same.\n",
    "\n",
    "# At this point, q and k are totally independant values.  They were computed \n",
    "# using matrix multiplies on different random weights.  Eventually these weights\n",
    "# will be determined through training.  Perhaps the key weights for the\n",
    "# 'a' token will map to \"I'm a vowel\".  And the query weights for the 'a' token\n",
    "# will map to \"Looking for types of consanants\".  But at this point they aren't\n",
    "# comapred with any other tokens yet.  So next step is to align them.\n",
    "\n",
    "# Those resulting outputs of the net are multiplied together.  This provides\n",
    "# a cross-coupling between tokens.  Where there is high affinity between k and q\n",
    "# we will find high weights, and where they are not well aligned, there will be \n",
    "# low values.  This is how dot product works.\n",
    "wei = q @ k.transpose(-2, -1)  # (B, T, 16) @ (B, 16, T) --> (B, T, T)\n",
    "\n",
    "# So wei is a much better weight matrix than if we had started with zeors and\n",
    "# done averages.  The values in the matrix are based on how much each token\n",
    "# pays \"attention\" to any other token in the T row.  \n",
    "tril = torch.tril(torch.ones(T, T))\n",
    "wei = wei.masked_fill(tril == 0, float('-inf'))  # Now mask to hide the future tokens.\n",
    "wei = F.softmax(wei, dim=-1)\n",
    "# At this point wei is the weighting between tokens.\n",
    "print(\"wei[0]:\")\n",
    "print(wei[0])\n",
    "# Note that they are not uniform values to do an average any more.\n",
    "print(f\"wei shape: {wei.shape}\")\n",
    "\n",
    "out = wei @ x\n",
    "print(f\"out shape: {out.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 8, 16])\n"
     ]
    }
   ],
   "source": [
    "# Catch: we don't put the raw x in a multiply with the weights.  We compute\n",
    "# a value for this that is the same size as our head:\n",
    "value = nn.Linear(C, head_size, bias=False) # map 32 inputs to 16 outputs using lots of neurons\n",
    "v = value(x)  # ie run the neural net from 32 -> 16\n",
    "\n",
    "out = wei @ v\n",
    "print(out.shape)  # Now output is the size of the attention head (B, T, 16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.1925, 0.1426, 0.2351, 0.1426, 0.2872])\n",
      "tensor([0.0326, 0.0030, 0.1615, 0.0030, 0.8000])\n"
     ]
    }
   ],
   "source": [
    "# Catch 2: The values we feed into softmax need to be fairly diffuse.  If they\n",
    "# start to get large (ie far from the 0 to 1 range) then softmax will start\n",
    "# to converge to just highlighting whichever value happens to be the biggest.  \n",
    "# Example good vector:\n",
    "test1 = torch.softmax(torch.tensor([0.1, -0.2, 0.3, -0.2, 0.5]),   dim=-1)\n",
    "test2 = torch.softmax(torch.tensor([0.1, -0.2, 0.3, -0.2, 0.5])*8, dim=-1)\n",
    "print(test1)\n",
    "print(test2)\n",
    "# Note how the second one is tending toward a one-hot encoding, which we don't want.\n",
    "# We want the weights to stay closer to the 0 to 1 range.  So to fix this we are\n",
    "# going to do "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
